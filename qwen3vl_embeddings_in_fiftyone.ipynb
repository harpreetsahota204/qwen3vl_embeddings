{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen3-VL-Embedding in FiftyOne\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/qwen3vl_embeddings/blob/main/qwen3vl_embeddings_in_fiftyone.ipynb)\n",
        "\n",
        "This notebook demonstrates how to use [Qwen3-VL-Embedding](https://huggingface.co/Qwen/Qwen3-VL-Embedding-2B) with [FiftyOne](https://docs.voxel51.com/) for multimodal embeddings, text-to-media similarity search, and zero-shot classification.\n",
        "\n",
        "Qwen3-VL-Embedding maps text, images, and video into a unified representation space, enabling powerful cross-modal retrieval and understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Install the required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1kzakX23jOH"
      },
      "outputs": [],
      "source": [
        "!pip install -q fiftyone decord qwen-vl-utils transformers torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note, you should install flash attention 2 for faster inference speed.\n",
        "\n",
        "## Load the Model\n",
        "\n",
        "Register the remote model source and load the Qwen3-VL-Embedding model. The 2B parameter variant offers a good balance of quality and speed; an 8B variant is also available for higher quality embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjCFwODi5Gyx"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "# Register the model source\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/qwen3vl_embeddings\",\n",
        "    overwrite=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JOiPdUT5G7F"
      },
      "outputs": [],
      "source": [
        "# Load Qwen3-VL model\n",
        "model = foz.load_zoo_model(\"Qwen/Qwen3-VL-Embedding-2B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Video Dataset\n",
        "\n",
        "Qwen3-VL-Embedding can generate embeddings for video content by sampling frames at a configurable FPS. Set `media_type=\"video\"` to process video datasets.\n",
        "\n",
        "We'll load a sample video dataset from the Hugging Face Hub and compute embeddings for each video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.media_type = \"video\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSo82FrW5G3Z"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "dataset = load_from_hub(\n",
        "    \"harpreetsahota/random_short_videos\",\n",
        "    name=\"random_short_videos\",\n",
        "    overwrite=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4JQBQye5G-j"
      },
      "outputs": [],
      "source": [
        "dataset.compute_embeddings(\n",
        "    model,\n",
        "    embeddings_field=\"qwen_embeddings\",\n",
        "    skip_failures=False,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you don't want to run inference and just want to see the results you can download the following dataset. However, you will need to make sure you have registered and loaded the zoo model as shown above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "load_from_hub(\"harpreetsahota/testing_qwen3vl_embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text-to-Video Similarity Search\n",
        "\n",
        "Build a similarity index to enable natural language search over your video dataset. Once indexed, you can find videos matching text queries like \"a person cooking in a kitchen\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMaP3ejR5HCf"
      },
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "# Build similarity index\n",
        "sim = fob.compute_similarity(\n",
        "    dataset,\n",
        "    model=\"Qwen/Qwen3-VL-Embedding-2B\",\n",
        "    brain_key=\"qwen_video_sim\",\n",
        "    embeddings=\"qwen_embeddings\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedding Visualization\n",
        "\n",
        "Use UMAP to project the high-dimensional embeddings into 2D for visualization. This helps you explore the semantic structure of your dataset in the FiftyOne App."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X66OfQie5HFl"
      },
      "outputs": [],
      "source": [
        "# Compute UMAP visualization\n",
        "results = fob.compute_visualization(\n",
        "    dataset,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"qwen_video_viz\",\n",
        "    embeddings=\"qwen_embeddings\",\n",
        "    num_dims=2\n",
        ")\n",
        "\n",
        "print(\"UMAP visualization computed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-Shot Classification\n",
        "\n",
        "Classify videos using text prompts without any training. Define a list of classes and an optional text prompt prefix, then apply the model to generate predictions based on embedding similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rnnv2FqM-2M7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Configure model for classification\n",
        "model.classes = [\n",
        "    \"children\", \n",
        "    \"babies\", \n",
        "    \"people exercising\", \n",
        "    \"bottle opening\", \n",
        "    \"pets or animals\", \n",
        "    \"cartoon\",\n",
        "    \"a door opening\",\n",
        "    \"person sleeping\",\n",
        "    \"undetermined activity\"]\n",
        "\n",
        "model.text_prompt = \"A video of \"\n",
        "\n",
        "# Apply zero-shot classification\n",
        "dataset.apply_model(model, label_field=\"zero_shot_classification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explore in FiftyOne App\n",
        "\n",
        "Launch the FiftyOne App to explore your dataset, view the UMAP visualization, run similarity searches, and inspect the zero-shot classification results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXWywBIZ-r4e"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset, auto=False)\n",
        "session.url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Dataset\n",
        "\n",
        "The same model can be used for image datasets by switching `media_type` to `\"image\"`. This allows you to reuse a single loaded model for both video and image workflows without reloading weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-Ub6ojCBDtE"
      },
      "source": [
        "Load the FiftyOne quickstart dataset, which contains 200 images with various object detection annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.zoo as foz\n",
        "\n",
        "image_dataset = foz.load_zoo_dataset(\"quickstart\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.media_type = \"image\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_dataset.compute_embeddings(\n",
        "    model,\n",
        "    embeddings_field=\"qwen_embeddings\",\n",
        "    skip_failures=False,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text-to-Image Similarity Search\n",
        "\n",
        "Build a similarity index for the image dataset using the same model. You can now search for images using natural language queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "# Build similarity index\n",
        "sim = fob.compute_similarity(\n",
        "    image_dataset,\n",
        "    model=\"Qwen/Qwen3-VL-Embedding-2B\",\n",
        "    brain_key=\"qwen_img_sim\",\n",
        "    embeddings=\"qwen_embeddings\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedding Visualization\n",
        "\n",
        "Visualize the image embeddings with UMAP to explore semantic clusters in your image dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute UMAP visualization\n",
        "results = fob.compute_visualization(\n",
        "    image_dataset,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"qwen_img_viz\",\n",
        "    embeddings=\"qwen_embeddings\",\n",
        "    num_dims=2\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fiftyone",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
